{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv1D, MaxPooling1D # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras.losses import BinaryCrossentropy # type: ignore\n",
    "from tensorflow.keras.metrics import BinaryAccuracy # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "from sincnet_tensorflow import SincConv1D, LayerNorm\n",
    "import random\n",
    "import tensorflow_hub as hub \n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio_data, sr):\n",
    "    \"\"\"Apply augmentation techniques to the audio data.\"\"\"\n",
    "    audio_data_augmented = librosa.effects.time_stretch(audio_data, rate=0.8)\n",
    "    # pitch_shift_amount = np.random.uniform(low=-2.0, high=2.0)\n",
    "    # audio_data_augmented = librosa.effects.pitch_shift(audio_data_augmented, sr=sr, n_steps=pitch_shift_amount)\n",
    "    \n",
    "    mask_start = np.random.randint(0, len(audio_data_augmented) - 50)\n",
    "    audio_data_augmented[mask_start:mask_start + 50] = 0\n",
    "\n",
    "    mask_start_freq = np.random.randint(0, int(len(audio_data_augmented) / 2) - 10)\n",
    "    audio_data_augmented[mask_start_freq:mask_start_freq + 10] = 0\n",
    "\n",
    "    # new_sr = np.random.randint(16000, 22050)\n",
    "    # audio_data_augmented = librosa.resample(audio_data_augmented, orig_sr=sr, target_sr=new_sr)\n",
    "    \n",
    "    return audio_data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, augmentation=False):\n",
    "    \"\"\"Extract features from audio file, optionally with augmentation.\"\"\"\n",
    "    audio_data, sr = librosa.load(file_path, sr=None)\n",
    "    if augmentation:\n",
    "        audio_data = augment_audio(audio_data, sr)\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40, n_fft= 512)\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr, n_fft=512)\n",
    "    chroma_mean = np.mean(chroma.T, axis=0)\n",
    "\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_fft= 512)\n",
    "    mel_spectrogram_mean = np.mean(librosa.power_to_db(mel_spectrogram).T, axis=0)\n",
    "\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, n_fft= 512)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n",
    "\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio_data), sr=sr)\n",
    "    tonnetz_mean = np.mean(tonnetz.T, axis=0)\n",
    "\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio_data)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate.T, axis=0)\n",
    "\n",
    "    features = np.hstack([mfccs_mean, chroma_mean, mel_spectrogram_mean, spectral_contrast_mean, tonnetz_mean, zero_crossing_rate_mean])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    \"\"\"Load and preprocess data from the dataset path.\"\"\"\n",
    "    features = []\n",
    "    features_original = []\n",
    "\n",
    "    for label in ['healthy', 'disease']:\n",
    "        folder_path = os.path.join(dataset_path, label)\n",
    "        files = [file for file in os.listdir(folder_path) if file.endswith('.wav')]\n",
    "        for file_name in tqdm(files, desc=f'Processing {label} files'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            features_original.append({'feature': extract_features(file_path, augmentation=False), 'class': label})\n",
    "            features.append({'feature': extract_features(file_path, augmentation=False), 'class': label})\n",
    "            features.append({'feature': extract_features(file_path, augmentation=True), 'class': label})\n",
    "    \n",
    "    df_original = pd.DataFrame(features_original)\n",
    "    df_augmented = pd.DataFrame(features)\n",
    "\n",
    "    return df_original, df_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset folder of vowel /a/ files\n",
    "dataset_path_a = 'VCC_a'\n",
    "df_original_a, df_augmented_a = load_data(dataset_path_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset folder of vowel /i/ files\n",
    "dataset_path_i = 'VCC_i'\n",
    "df_original_i, df_augmented_i = load_data(dataset_path_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset folder of vowel /u/ files\n",
    "dataset_path_u = 'VCC_u'\n",
    "df_original_u, df_augmented_u = load_data(dataset_path_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset folder of vowel /iau/ files\n",
    "dataset_path_iau = 'VCC_iau'\n",
    "# df_original_iau, df_augmented_iau = load_data(dataset_path_iau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into Train-Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for vowel /a/\n",
    "X_train_original_a, X_test_original_a, y_train_original_a, y_test_original_a = train_test_split(df_original_a['feature'].values.tolist(), df_original_a['class'].values, test_size=0.5, random_state=42)\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(df_augmented_a['feature'].values.tolist(), df_augmented_a['class'].values, test_size=0.3, stratify=df_augmented_a['class'].values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for vowel /i/\n",
    "X_train_original_i, X_test_original_i, y_train_original_i, y_test_original_i = train_test_split(df_original_i['feature'].values.tolist(), df_original_i['class'].values, test_size=0.5, random_state=42)\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(df_augmented_i['feature'].values.tolist(), df_augmented_i['class'].values, test_size=0.3, stratify=df_augmented_i['class'].values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for vowel /u/\n",
    "X_train_original_u, X_test_original_u, y_train_original_u, y_test_original_u = train_test_split(df_original_u['feature'].values.tolist(), df_original_u['class'].values, test_size=0.5, random_state=42)\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(df_augmented_u['feature'].values.tolist(), df_augmented_u['class'].values, test_size=0.3, stratify=df_augmented_u['class'].values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the data for vowel /iau/\n",
    "# X_train_original_iau, X_test_original_iau, y_train_original_iau, y_test_original_iau = train_test_split(df_original_iau['feature'].values.tolist(), df_original_iau['class'].values, test_size=0.5, random_state=42)\n",
    "# X_train_iau, X_test_iau, y_train_iau, y_test_iau = train_test_split(df_augmented_iau['feature'].values.tolist(), df_augmented_iau['class'].values, test_size=0.2, stratify=df_augmented_iau['class'].values, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data conversion to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data for vowel /a/ to numpy arrays \n",
    "X_train_original_a = np.array(X_train_original_a)\n",
    "X_test_original_a = np.array(X_test_original_a)\n",
    "X_train_a = np.array(X_train_a)\n",
    "X_test_a = np.array(X_test_a)\n",
    "X_train_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data for vowel /i/ to numpy arrays \n",
    "X_train_original_i = np.array(X_train_original_i)\n",
    "X_test_original_i = np.array(X_test_original_i)\n",
    "X_train_i = np.array(X_train_i)\n",
    "X_test_i = np.array(X_test_i)\n",
    "X_train_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data for vowel /u/ to numpy arrays \n",
    "X_train_original_u = np.array(X_train_original_u)\n",
    "X_test_original_u = np.array(X_test_original_u)\n",
    "X_train_u = np.array(X_train_u)\n",
    "X_test_u = np.array(X_test_u)\n",
    "X_train_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the data for vowel /iau/ to numpy arrays \n",
    "# X_train_original_iau = np.array(X_train_original_iau)\n",
    "# X_test_original_iau = np.array(X_test_original_iau)\n",
    "# X_train_iau = np.array(X_train_iau)\n",
    "# X_test_iau = np.array(X_test_iau)\n",
    "# X_train_iau.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for the vowel /a/ data\n",
    "y_train_original_a = (np.array(y_train_original_a) == 'disease').astype(int)\n",
    "y_test_original_a = (np.array(y_test_original_a) == 'disease').astype(int)\n",
    "y_train_a = (np.array(y_train_a) == 'disease').astype(int)\n",
    "y_test_a = (np.array(y_test_a) == 'disease').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for the vowel /i/ data\n",
    "y_train_original_i = (np.array(y_train_original_i) == 'disease').astype(int)\n",
    "y_test_original_i = (np.array(y_test_original_i) == 'disease').astype(int)\n",
    "y_train_i = (np.array(y_train_i) == 'disease').astype(int)\n",
    "y_test_i = (np.array(y_test_i) == 'disease').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for the vowel /u/ data\n",
    "y_train_original_u = (np.array(y_train_original_u) == 'disease').astype(int)\n",
    "y_test_original_u = (np.array(y_test_original_u) == 'disease').astype(int)\n",
    "y_train_u = (np.array(y_train_u) == 'disease').astype(int)\n",
    "y_test_u = (np.array(y_test_u) == 'disease').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode labels for the vowel /iau/ data\n",
    "# y_train_original_iau = (np.array(y_train_original_iau) == 'disease').astype(int)\n",
    "# y_test_original_iau = (np.array(y_test_original_iau) == 'disease').astype(int)\n",
    "# y_train_iau = (np.array(y_train_iau) == 'disease').astype(int)\n",
    "# y_test_iau = (np.array(y_test_iau) == 'disease').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and fit Logistic Regression model for vowel /a/ data\n",
    "logreg_model_a = LogisticRegression(max_iter=1000)\n",
    "logreg_model_a.fit(X_train_a, y_train_a)\n",
    "\n",
    "# Evaluate on test set for vowel /a/ data\n",
    "logreg_accuracy_a = logreg_model_a.score(X_test_original_a, y_test_original_a)\n",
    "print(f'Logistic Regression Test Accuracy for vowel /a/ dataset: {logreg_accuracy_a:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Logistic Regression model for vowel /i/ data\n",
    "logreg_model_i = LogisticRegression(max_iter=1000)\n",
    "logreg_model_i.fit(X_train_i, y_train_i)\n",
    "\n",
    "# Evaluate on test set for vowel /a/ data\n",
    "logreg_accuracy_i = logreg_model_i.score(X_test_original_i, y_test_original_i)\n",
    "print(f'Logistic Regression Test Accuracy for vowel /i/ dataset: {logreg_accuracy_i:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Logistic Regression model for vowel /u/ data\n",
    "logreg_model_u = LogisticRegression(max_iter=1000)\n",
    "logreg_model_u.fit(X_train_u, y_train_u)\n",
    "\n",
    "# Evaluate on test set for vowel /u/ data\n",
    "logreg_accuracy_u = logreg_model_u.score(X_test_original_u, y_test_original_u)\n",
    "print(f'Logistic Regression Test Accuracy for vowel /u/ dataset: {logreg_accuracy_u:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and fit Logistic Regression model for vowel /iau/ data\n",
    "# logreg_model_iau = LogisticRegression(max_iter=1000)\n",
    "# logreg_model_iau.fit(X_train_iau, y_train_iau)\n",
    "\n",
    "# # Evaluate on test set for vowel /iau/ data\n",
    "# logreg_accuracy_iau = logreg_model_iau.score(X_test_original_iau, y_test_original_iau)\n",
    "# print(f'Logistic Regression Test Accuracy for vowel /iau/ dataset: {logreg_accuracy_iau:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and fit Random Forest model for vowel /a/ data\n",
    "rf_model_a = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_a.fit(X_train_a, y_train_a)\n",
    "\n",
    "# Evaluate on test set for vowel /a/ data\n",
    "rf_accuracy_a = rf_model_a.score(X_test_original_a, y_test_original_a)\n",
    "print(f'Random Forest Test Accuracy for vowel /a/ dataset: {rf_accuracy_a:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Random Forest model for vowel /i/ data\n",
    "rf_model_i = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_i.fit(X_train_i, y_train_i)\n",
    "\n",
    "# Evaluate on test set for vowel /i/ data\n",
    "rf_accuracy_i = rf_model_i.score(X_test_original_i, y_test_original_i)\n",
    "print(f'Random Forest Test Accuracy for vowel /i/ dataset: {rf_accuracy_i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Random Forest model for vowel /u/ data\n",
    "rf_model_u = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_u.fit(X_train_u, y_train_u)\n",
    "\n",
    "# Evaluate on test set for vowel /u/ data\n",
    "rf_accuracy_u = rf_model_u.score(X_test_original_u, y_test_original_u)\n",
    "print(f'Random Forest Test Accuracy for vowel /u/ dataset: {rf_accuracy_u:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and fit Random Forest model for vowel /iau/ data\n",
    "# rf_model_iau = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model_iau.fit(X_train_iau, y_train_iau)\n",
    "\n",
    "# # Evaluate on test set for vowel /iau/ data\n",
    "# rf_accuracy_iau = rf_model_iau.score(X_test_original_iau, y_test_original_iau)\n",
    "# print(f'Random Forest Test Accuracy for vowel /iau/ dataset: {rf_accuracy_iau:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and fit SVM model for vowel /a/ data\n",
    "svm_model_a = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_model_a.fit(X_train_a, y_train_a)\n",
    "\n",
    "# Evaluate on test set for vowel /a/ data\n",
    "svm_accuracy_a = svm_model_a.score(X_test_original_a, y_test_original_a)\n",
    "print(f'SVM Test Accuracy for vowel /a/ dataset: {svm_accuracy_a:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit SVM model for vowel /i/ data\n",
    "svm_model_i = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_model_i.fit(X_train_i, y_train_i)\n",
    "\n",
    "# Evaluate on test set for vowel /i/ data\n",
    "svm_accuracy_i = svm_model_i.score(X_test_original_i, y_test_original_i)\n",
    "print(f'SVM Test Accuracy for vowel /i/ dataset: {svm_accuracy_i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit SVM model for vowel /u/ data\n",
    "svm_model_u = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_model_u.fit(X_train_u, y_train_u)\n",
    "\n",
    "# Evaluate on test set for vowel /u/ data\n",
    "svm_accuracy_u = svm_model_u.score(X_test_original_u, y_test_original_u)\n",
    "print(f'SVM Test Accuracy for vowel /u/ dataset: {svm_accuracy_u:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and fit SVM model for vowel /iau/ data\n",
    "# svm_model_iau = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "# svm_model_iau.fit(X_train_iau, y_train_iau)\n",
    "\n",
    "# # Evaluate on test set for vowel /iau/ data\n",
    "# svm_accuracy_iau = svm_model_iau.score(X_test_original_iau, y_test_original_iau)\n",
    "# print(f'SVM Test Accuracy for vowel /iau/ dataset: {svm_accuracy_iau:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Array reshaping for SincNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape /a/ vowel data for SincNet\n",
    "X_train_a = X_train_a.reshape(-1, X_train_a.shape[1], 1)\n",
    "X_test_a = X_test_a.reshape(-1, X_test_a.shape[1], 1)\n",
    "X_test_original_a = X_test_original_a.reshape(-1, X_test_original_a.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape /i/ vowel data for SincNet\n",
    "X_train_i = X_train_i.reshape(-1, X_train_i.shape[1], 1)\n",
    "X_test_i = X_test_i.reshape(-1, X_test_i.shape[1], 1)\n",
    "X_test_original_i = X_test_original_i.reshape(-1, X_test_original_i.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape /u/ vowel data for SincNet\n",
    "X_train_u = X_train_u.reshape(-1, X_train_u.shape[1], 1)\n",
    "X_test_u = X_test_u.reshape(-1, X_test_u.shape[1], 1)\n",
    "X_test_original_u = X_test_original_u.reshape(-1, X_test_original_u.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape /iau/ vowel data for SincNet\n",
    "# X_train_iau = X_train_iau.reshape(-1, X_train_iau.shape[1], 1)\n",
    "# X_test_iau = X_test_iau.reshape(-1, X_test_iau.shape[1], 1)\n",
    "# X_test_original_iau = X_test_original_iau.reshape(-1, X_test_original_iau.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SincNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sincnet_model(input_shape):\n",
    "    model = Sequential([\n",
    "        SincConv1D(N_filt=80, Filt_dim=11, fs=16000, stride=1, padding=\"VALID\", input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=3),\n",
    "        Conv1D(60, kernel_size=5, padding=\"valid\", activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=3),\n",
    "        Conv1D(60, kernel_size=5, padding=\"valid\", activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=3),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = build_sincnet_model((X_train_a.shape[1], 1))\n",
    "model_i = build_sincnet_model((X_train_i.shape[1], 1))\n",
    "model_u = build_sincnet_model((X_train_u.shape[1], 1))\n",
    "# model_iau = build_sincnet_model((X_train_iau.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.summary()\n",
    "model_i.summary()\n",
    "model_u.summary()\n",
    "# model_iau.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_a.fit(X_train_a, y_train_a, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "model_i.fit(X_train_i, y_train_i, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "model_u.fit(X_train_u, y_train_u, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "# model_iau.fit(X_train_iau, y_train_iau, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set of /a/ vowel dataset\n",
    "loss_a, accuracy_a = model_a.evaluate(X_test_original_a, y_test_original_a)\n",
    "print(f'Test accuracy on original test set of /a/ vowel dataset: {accuracy_a:.4f}')\n",
    "print()\n",
    "# Evaluate model on training data of /a/ vowel dataset\n",
    "train_loss_a, train_accuracy_a = model_a.evaluate(X_train_a, y_train_a)\n",
    "print(f'Train accuracy on /a/ vowel dataset: {train_accuracy_a:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set of /i/ vowel dataset\n",
    "loss_i, accuracy_i = model_i.evaluate(X_test_original_i, y_test_original_i)\n",
    "print(f'Test accuracy on original test set of /i/ vowel dataset: {accuracy_i:.4f}')\n",
    "print()\n",
    "# Evaluate model on training data of /i/ vowel dataset\n",
    "train_loss_i, train_accuracy_i = model_i.evaluate(X_train_i, y_train_i)\n",
    "print(f'Train accuracy on /i/ vowel dataset: {train_accuracy_i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set of /u/ vowel dataset\n",
    "loss_u, accuracy_u = model_u.evaluate(X_test_original_u, y_test_original_u)\n",
    "print(f'Test accuracy on original test set of /u/ vowel dataset: {accuracy_u:.4f}')\n",
    "print()\n",
    "# Evaluate model on training data of /u/ vowel dataset\n",
    "train_loss_u, train_accuracy_u = model_u.evaluate(X_train_u, y_train_u)\n",
    "print(f'Train accuracy on /u/ vowel dataset: {train_accuracy_u:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate on the test set of /iau/ vowel dataset\n",
    "# loss_iau, accuracy_iau = model_iau.evaluate(X_test_original_iau, y_test_original_iau)\n",
    "# print(f'Test accuracy on original test set of /iau/ vowel dataset: {accuracy_iau:.4f}')\n",
    "# print()\n",
    "# # Evaluate model on training data of /iau/ vowel dataset\n",
    "# train_loss_iau, train_accuracy_iau = model_iau.evaluate(X_train_iau, y_train_iau)\n",
    "# print(f'Train accuracy on /iau/ vowel dataset: {train_accuracy_iau:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for original test set of vowel /a/ dataset\n",
    "y_pred_test_a = model_a.predict(X_test_original_a)\n",
    "y_pred_binary_a = (y_pred_test_a > 0.5).astype(int)\n",
    "cm_a = confusion_matrix(y_test_original_a, y_pred_binary_a)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_a, annot=True, cmap='Blues', fmt='g', xticklabels=['Healthy', 'Disease'], yticklabels=['Healthy', 'Disease'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Original Test Set of vowel /a/ dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for original test set of vowel /i/ dataset\n",
    "y_pred_test_i = model_i.predict(X_test_original_i)\n",
    "y_pred_binary_i = (y_pred_test_i > 0.5).astype(int)\n",
    "cm_i = confusion_matrix(y_test_original_i, y_pred_binary_i)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_i, annot=True, cmap='Blues', fmt='g', xticklabels=['Healthy', 'Disease'], yticklabels=['Healthy', 'Disease'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Original Test Set of vowel /i/ dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for original test set of vowel /u/ dataset\n",
    "y_pred_test_u = model_u.predict(X_test_original_u)\n",
    "y_pred_binary_u = (y_pred_test_u > 0.5).astype(int)\n",
    "cm_u = confusion_matrix(y_test_original_u, y_pred_binary_u)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_u, annot=True, cmap='Blues', fmt='g', xticklabels=['Healthy', 'Disease'], yticklabels=['Healthy', 'Disease'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Original Test Set of vowel /u/ dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix for original test set of vowel /iau/ dataset\n",
    "# y_pred_test_iau = model_iau.predict(X_test_original_iau)\n",
    "# y_pred_binary_iau = (y_pred_test_iau > 0.5).astype(int)\n",
    "# cm_iau = confusion_matrix(y_test_original_iau, y_pred_binary_iau)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm_iau, annot=True, cmap='Blues', fmt='g', \n",
    "#             xticklabels=['Healthy', 'Disease'], yticklabels=['Healthy', 'Disease'])\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix - Original Test Set of vowel /iau/ dataset')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for training data of vowel /a/ dataset\n",
    "y_pred_train_a = (model_a.predict(X_train_a) > 0.5).astype(int)\n",
    "cm_train_a = confusion_matrix(y_train_a, y_pred_train_a)\n",
    "\n",
    "classes = ['Healthy', 'Disease']\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_train_a, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Training Data of vowel /a/ dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for training data of vowel /i/ dataset\n",
    "y_pred_train_i = (model_i.predict(X_train_i) > 0.5).astype(int)\n",
    "cm_train_i = confusion_matrix(y_train_i, y_pred_train_i)\n",
    "\n",
    "classes = ['Healthy', 'Disease']\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_train_i, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Training Data of vowel /i/ dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for training data of vowel /u/ dataset\n",
    "y_pred_train_u = (model_u.predict(X_train_u) > 0.5).astype(int)\n",
    "cm_train_u = confusion_matrix(y_train_u, y_pred_train_u)\n",
    "\n",
    "classes = ['Healthy', 'Disease']\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_train_u, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Training Data of vowel /u/ dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion matrix for training data of vowel /iau/ dataset\n",
    "# y_pred_train_iau = (model_iau.predict(X_train_iau) > 0.5).astype(int)\n",
    "# cm_train_iau = confusion_matrix(y_train_iau, y_pred_train_iau)\n",
    "\n",
    "# classes = ['Healthy', 'Disease']\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm_train_iau, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix - Training Data of vowel /iau/ dataset')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
